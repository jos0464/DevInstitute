<!DOCTYPE html>
<html lang="fr">
<head>
<meta charset="UTF-8">
<title>Fiche Formules Compl√®te IA / ML / DL</title>
<style>
    body { font-family: Arial, sans-serif; margin: 20px; background-color: #f9f9f9; }
    h1, h2 { color: #333; }
    table { width: 100%; border-collapse: collapse; margin-bottom: 30px; background-color: #fff; }
    th, td { border: 1px solid #ccc; padding: 8px 12px; text-align: left; vertical-align: top; }
    th { background-color: #D0E8C0; }
    .section th { background-color: #C6D7F5; }
    .loss th { background-color: #F5D6C6; }
    .eval th { background-color: #F5F5C6; }
    code { background-color: #eee; padding: 2px 4px; border-radius: 4px; display:block; }
</style>
</head>
<body>
<h1>üìä Fiche Formules Compl√®te IA / ML / DL ‚Äî FR / EN / Python / Utilit√© / Domaine</h1>

<h2>1Ô∏è‚É£ R√©gression Lin√©aire</h2>
<table class="section">
<thead>
<tr>
<th>Concept (FR)</th>
<th>Term (EN)</th>
<th>Formule</th>
<th>Python</th>
<th>Utilit√© / Explication</th>
<th>Domaine</th>
</tr>
</thead>
<tbody>
<tr><td>Mod√®le pr√©diction (simple)</td><td>Prediction Model (Simple)</td><td>≈∑ = w¬∑x + b</td><td><code>y_pred = w*x + b</code></td><td>Pr√©dire la valeur d'une variable cible √† partir d'une seule variable</td><td>ML / IA</td></tr>
<tr><td>Mod√®le pr√©diction (multiple)</td><td>Prediction Model (Multiple)</td><td>≈∑ = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + b</td><td><code>y_pred = np.dot(X, w) + b</code></td><td>Pr√©dire la valeur d'une cible √† partir de plusieurs variables</td><td>ML / IA</td></tr>
<tr><td>Erreur r√©siduelle</td><td>Residual Error</td><td>e·µ¢ = y·µ¢ - ≈∑·µ¢</td><td><code>residual = y_true - y_pred</code></td><td>Mesurer l'√©cart entre la pr√©diction et la valeur r√©elle</td><td>ML / IA</td></tr>
<tr><td>MSE</td><td>Mean Squared Error</td><td>MSE = 1/n Œ£(y·µ¢ - ≈∑·µ¢)¬≤</td><td><code>mse = np.mean((y_true - y_pred)**2)</code></td><td>Quantifier l'erreur moyenne au carr√©</td><td>ML / DL</td></tr>
<tr><td>MAE</td><td>Mean Absolute Error</td><td>MAE = 1/n Œ£|y·µ¢ - ≈∑·µ¢|</td><td><code>mae = np.mean(np.abs(y_true - y_pred))</code></td><td>Erreur moyenne absolue, moins sensible aux valeurs extr√™mes</td><td>ML / DL</td></tr>
<tr><td>RMSE</td><td>Root Mean Squared Error</td><td>RMSE = ‚àöMSE</td><td><code>rmse = np.sqrt(np.mean((y_true - y_pred)**2))</code></td><td>Erreur standard, m√™me unit√© que les donn√©es</td><td>ML / DL</td></tr>
<tr><td>Fonction de co√ªt</td><td>Cost / Loss Function</td><td>J(w,b) = 1/2n Œ£(y·µ¢ - ≈∑·µ¢)¬≤</td><td><code>loss = 0.5*np.mean((y_true - y_pred)**2)</code></td><td>√âvaluer la performance globale du mod√®le</td><td>ML / DL</td></tr>
</tbody>
</table>

<h2>2Ô∏è‚É£ Descente de Gradient / Optimisation</h2>
<table class="loss">
<thead>
<tr>
<th>Concept (FR)</th>
<th>Term (EN)</th>
<th>Formule</th>
<th>Python</th>
<th>Utilit√© / Explication</th>
<th>Domaine</th>
</tr>
</thead>
<tbody>
<tr><td>Gradient (partiel)</td><td>Partial Gradient</td><td>‚àÇJ/‚àÇw = -1/n Œ£(y·µ¢ - ≈∑·µ¢)x·µ¢ ; ‚àÇJ/‚àÇb = -1/n Œ£(y·µ¢ - ≈∑·µ¢)</td><td><code>dw = -(1/n)*np.sum((y_true - y_pred)*x)<br>db = -(1/n)*np.sum(y_true - y_pred)</code></td><td>Indique la direction et l'ampleur du changement des param√®tres</td><td>ML / DL</td></tr>
<tr><td>Descente de gradient</td><td>Gradient Descent</td><td>w = w - Œ± ‚àÇJ/‚àÇw ; b = b - Œ± ‚àÇJ/‚àÇb</td><td><code>w = w - alpha*dw ; b = b - alpha*db</code></td><td>Optimiser les param√®tres pour r√©duire la perte</td><td>ML / DL</td></tr>
<tr><td>Taux d‚Äôapprentissage</td><td>Learning Rate</td><td>Œ±</td><td><code>alpha = 0.01</code></td><td>Contr√¥le la vitesse de mise √† jour des param√®tres</td><td>ML / DL</td></tr>
<tr><td>Hyperparam√®tres</td><td>Hyperparameters</td><td>Param√®tres fix√©s avant entra√Ænement</td><td><code># alpha, iterations</code></td><td>Influencent la vitesse et la qualit√© de l'apprentissage</td><td>ML / DL</td></tr>
</tbody>
</table>

<h2>3Ô∏è‚É£ Statistiques de base et avanc√©es</h2>
<table class="eval">
<thead>
<tr>
<th>Concept (FR)</th>
<th>Term (EN)</th>
<th>Formule</th>
<th>Python</th>
<th>Utilit√© / Explication</th>
<th>Domaine</th>
</tr>
</thead>
<tbody>
<tr><td>Moyenne</td><td>Mean</td><td>xÃÑ = 1/n Œ£x·µ¢</td><td><code>mean = np.mean(x)</code></td><td>Valeur centrale d'un jeu de donn√©es</td><td>ML / IA</td></tr>
<tr><td>Variance</td><td>Variance</td><td>Var(X) = 1/n Œ£(x·µ¢ - xÃÑ)¬≤</td><td><code>var = np.var(x)</code></td><td>Mesure de dispersion autour de la moyenne</td><td>ML / IA</td></tr>
<tr><td>√âcart-type</td><td>Standard Deviation</td><td>œÉ = ‚àöVar(X)</td><td><code>std = np.std(x)</code></td><td>Dispersion exprim√©e dans la m√™me unit√© que les donn√©es</td><td>ML / IA</td></tr>
<tr><td>Covariance</td><td>Covariance</td><td>Cov(X,Y) = 1/n Œ£(x·µ¢ - xÃÑ)(y·µ¢ - »≥)</td><td><code>cov = np.mean((x - np.mean(x))*(y - np.mean(y)))</code></td><td>Mesure la relation lin√©aire entre deux variables</td><td>ML / IA</td></tr>
<tr><td>Coefficient corr√©lation</td><td>Correlation Coefficient</td><td>r = Cov(X,Y)/(œÉ‚ÇìœÉ·µß)</td><td><code>r = np.corrcoef(x,y)[0,1]</code></td><td>Force et direction de la relation lin√©aire</td><td>ML / IA</td></tr>
<tr><td>M√©diane</td><td>Median</td><td>valeur qui s√©pare la moiti√© sup√©rieure et inf√©rieure des donn√©es</td><td><code>median = np.median(x)</code></td><td>Centre des donn√©es, moins sensible aux valeurs extr√™mes</td><td>ML / IA</td></tr>
<tr><td>Mode</td><td>Mode</td><td>valeur la plus fr√©quente</td><td><code>from scipy import stats<br>mode_val = stats.mode(x)</code></td><td>Valeur la plus fr√©quente</td><td>ML / IA</td></tr>
<tr><td>Quantile</td><td>Quantile</td><td>x_q tel que P(X ‚â§ x_q) = q</td><td><code>quantile_25 = np.quantile(x, 0.25)</code></td><td>Divise les donn√©es en intervalles √©gaux</td><td>ML / IA</td></tr>
<tr><td>Percentile</td><td>Percentile</td><td>p·µ¢ = valeur en dessous de laquelle i% des donn√©es se trouvent</td><td><code>percentile_90 = np.percentile(x, 90)</code></td><td>Identifie la position d‚Äôune donn√©e dans la distribution</td><td>ML / IA</td></tr>
<tr><td>Asym√©trie</td><td>Skewness</td><td>Skew = E[(X - Œº)¬≥] / œÉ¬≥</td><td><code>from scipy.stats import skew<br>skew_val = skew(x)</code></td><td>Mesure la sym√©trie de la distribution</td><td>ML / IA</td></tr>
<tr><td>Aplatissement</td><td>Kurtosis</td><td>Kurt = E[(X - Œº)‚Å¥] / œÉ‚Å¥</td><td><code>from scipy.stats import kurtosis<br>kurt_val = kurtosis(x)</code></td><td>Indique si la distribution est pointue ou aplatie</td><td>ML / IA</td></tr>
</tbody>
</table>

<h2>4Ô∏è‚É£ R√©gression Logistique / Classification</h2>
<table class="section">
<thead>
<tr>
<th>Concept (FR)</th>
<th>Term (EN)</th>
<th>Formule</th>
<th>Python</th>
<th>Utilit√© / Explication</th>
<th>Domaine</th>
</tr>
</thead>
<tbody>
<tr><td>Hypoth√®se Sigmo√Øde</td><td>Sigmoid Hypothesis</td><td>≈∑ = 1 / (1 + e^(- (w¬∑x + b)))</td><td><code>y_pred = sigmoid(np.dot(X, w) + b)</code></td><td>Calcul de la probabilit√© d‚Äôappartenance √† une classe</td><td>ML / DL</td></tr>
<tr><td>Fonction de co√ªt logistique</td><td>Log Loss</td><td>J(w,b) = -1/n Œ£ [y log(≈∑) + (1-y) log(1-≈∑)]</td><td><code>loss = -np.mean(y*np.log(y_pred)+(1-y)*np.log(1-y_pred))</code></td><td>Mesure l‚Äôerreur pour les mod√®les de classification binaire</td><td>ML / DL</td></tr>
</tbody>
</table>

<h2>5Ô∏è‚É£ R√©seaux de Neurones / Deep Learning</h2>
<table class="section">
<thead>
<tr>
<th>Concept (FR)</th>
<th>Term (EN)</th>
<th>Formule</th>
<th>Python</th>
<th>Utilit√© / Explication</th>
<th>Domaine</th>
</tr>
</thead>
<tbody>
<tr><td>Perceptron</td><td>Perceptron</td><td>y = f(Œ£ w·µ¢x·µ¢ + b)</td><td><code>y = activation(np.dot(W, X) + b)</code></td><td>Unit√© de base d‚Äôun r√©seau de neurones</td><td>DL</td></tr>
<tr><td>Activation Sigmo√Øde</td><td>Sigmoid Activation</td><td>œÉ(z) = 1 / (1 + e^-z)</td><td><code>sigmoid = 1/(1+np.exp(-z))</code></td><td>Transforme la sortie en probabilit√©</td><td>DL</td></tr>
<tr><td>Activation ReLU</td><td>ReLU Activation</td><td>f(z) = max(0,z)</td><td><code>relu = np.maximum(0, z)</code></td><td>Active uniquement les valeurs positives</td><td>DL</td></tr>
<tr><td>Softmax</td><td>Softmax</td><td>œÉ(z·µ¢) = e^(z·µ¢)/Œ£ e^(z‚±º)</td><td><code>softmax = np.exp(z)/np.sum(np.exp(z))</code></td><td>Convertit vecteur en probabilit√©s pour classification multi-classes</td><td>DL</td></tr>
<tr><td>Cross-Entropy</td><td>Cross-Entropy</td><td>CE = - Œ£ y log(≈∑)</td><td><code>loss = -np.sum(y_true*np.log(y_pred))</code></td><td>Fonction de perte pour classification multi-classes</td><td>DL</td></tr>
<tr><td>Backpropagation</td><td>Backpropagation</td><td>Œîw = -Œ± ‚àÇJ/‚àÇw</td><td><code>w = w - alpha*dw</code></td><td>Algorithme pour ajuster les poids du r√©seau</td><td>DL</td></tr>
</tbody>
</table>
</body>
</html>
